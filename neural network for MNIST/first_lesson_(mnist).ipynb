{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first lesson (mnist).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y-AHVJ8a1_o"
      },
      "source": [
        "## import all libraries\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTAoWdwRgKT-"
      },
      "source": [
        "## download train and test dataset with normalize\n",
        "\n",
        "batch_size = 200 # size of one batch for train\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,)) # standard normalize for mnist \n",
        "                       ])),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LwkSBpvf9PP"
      },
      "source": [
        "# we use standard pytorch class nn.Module to create neural network\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        # initilization\n",
        "        super().__init__()\n",
        "        # neural network with 4 layers\n",
        "        self.fc1 = nn.Linear(28 * 28, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc3 = nn.Linear(200, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # principles\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyZ1Vjy6iNgB",
        "outputId": "a3d2368c-ac3e-492e-91a6-3f02aa21106d"
      },
      "source": [
        "linModel = LinearModel()\n",
        "print(linModel)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearModel(\n",
            "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb256QtJiQCM"
      },
      "source": [
        "# create SGD optimization\n",
        "optimizer = optim.SGD(linModel.parameters(), lr=0.05, momentum=0.9) # wo momentum\n",
        "\n",
        "# create loss func\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJOFLODSjIay",
        "outputId": "f49c8bb6-7293-443f-e944-81c1ac16570d"
      },
      "source": [
        "# train\n",
        "for epoch in range(10):\n",
        "   for batch_idx, (data, target) in enumerate(train_loader):\n",
        "       data, target = Variable(data), Variable(target)\n",
        "       data = data.view(-1, 28 * 28)\n",
        "       optimizer.zero_grad()\n",
        "       no = linModel(data)\n",
        "       loss = criterion(no, target)\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "       if batch_idx % 10 == 0:\n",
        "          print(\"Epoch №:\", epoch, \", percent of batch:\", float('{:.0f}'.format(((100. * batch_idx) / len(train_loader)))),\n",
        "                \", Loss: \", float('{:.6f}'.format(loss.item())))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch №: 0 , percent of batch: 0.0 , Loss:  2.9e-05\n",
            "Epoch №: 0 , percent of batch: 3.0 , Loss:  7.6e-05\n",
            "Epoch №: 0 , percent of batch: 7.0 , Loss:  4.5e-05\n",
            "Epoch №: 0 , percent of batch: 10.0 , Loss:  5.4e-05\n",
            "Epoch №: 0 , percent of batch: 13.0 , Loss:  6.9e-05\n",
            "Epoch №: 0 , percent of batch: 17.0 , Loss:  0.000172\n",
            "Epoch №: 0 , percent of batch: 20.0 , Loss:  7.5e-05\n",
            "Epoch №: 0 , percent of batch: 23.0 , Loss:  6e-05\n",
            "Epoch №: 0 , percent of batch: 27.0 , Loss:  5.8e-05\n",
            "Epoch №: 0 , percent of batch: 30.0 , Loss:  6.8e-05\n",
            "Epoch №: 0 , percent of batch: 33.0 , Loss:  4.8e-05\n",
            "Epoch №: 0 , percent of batch: 37.0 , Loss:  7.4e-05\n",
            "Epoch №: 0 , percent of batch: 40.0 , Loss:  3.5e-05\n",
            "Epoch №: 0 , percent of batch: 43.0 , Loss:  3.5e-05\n",
            "Epoch №: 0 , percent of batch: 47.0 , Loss:  4.7e-05\n",
            "Epoch №: 0 , percent of batch: 50.0 , Loss:  5.2e-05\n",
            "Epoch №: 0 , percent of batch: 53.0 , Loss:  4.9e-05\n",
            "Epoch №: 0 , percent of batch: 57.0 , Loss:  3e-05\n",
            "Epoch №: 0 , percent of batch: 60.0 , Loss:  5.3e-05\n",
            "Epoch №: 0 , percent of batch: 63.0 , Loss:  6.2e-05\n",
            "Epoch №: 0 , percent of batch: 67.0 , Loss:  3.7e-05\n",
            "Epoch №: 0 , percent of batch: 70.0 , Loss:  6.2e-05\n",
            "Epoch №: 0 , percent of batch: 73.0 , Loss:  1.9e-05\n",
            "Epoch №: 0 , percent of batch: 77.0 , Loss:  5e-05\n",
            "Epoch №: 0 , percent of batch: 80.0 , Loss:  3.5e-05\n",
            "Epoch №: 0 , percent of batch: 83.0 , Loss:  3.5e-05\n",
            "Epoch №: 0 , percent of batch: 87.0 , Loss:  8.3e-05\n",
            "Epoch №: 0 , percent of batch: 90.0 , Loss:  1.4e-05\n",
            "Epoch №: 0 , percent of batch: 93.0 , Loss:  5.4e-05\n",
            "Epoch №: 0 , percent of batch: 97.0 , Loss:  0.0001\n",
            "Epoch №: 1 , percent of batch: 0.0 , Loss:  7.5e-05\n",
            "Epoch №: 1 , percent of batch: 3.0 , Loss:  5.5e-05\n",
            "Epoch №: 1 , percent of batch: 7.0 , Loss:  4.5e-05\n",
            "Epoch №: 1 , percent of batch: 10.0 , Loss:  1.9e-05\n",
            "Epoch №: 1 , percent of batch: 13.0 , Loss:  4e-05\n",
            "Epoch №: 1 , percent of batch: 17.0 , Loss:  6e-05\n",
            "Epoch №: 1 , percent of batch: 20.0 , Loss:  2.5e-05\n",
            "Epoch №: 1 , percent of batch: 23.0 , Loss:  4.3e-05\n",
            "Epoch №: 1 , percent of batch: 27.0 , Loss:  5.2e-05\n",
            "Epoch №: 1 , percent of batch: 30.0 , Loss:  5.7e-05\n",
            "Epoch №: 1 , percent of batch: 33.0 , Loss:  5.4e-05\n",
            "Epoch №: 1 , percent of batch: 37.0 , Loss:  6.1e-05\n",
            "Epoch №: 1 , percent of batch: 40.0 , Loss:  5.7e-05\n",
            "Epoch №: 1 , percent of batch: 43.0 , Loss:  4.8e-05\n",
            "Epoch №: 1 , percent of batch: 47.0 , Loss:  7.7e-05\n",
            "Epoch №: 1 , percent of batch: 50.0 , Loss:  5.2e-05\n",
            "Epoch №: 1 , percent of batch: 53.0 , Loss:  5.8e-05\n",
            "Epoch №: 1 , percent of batch: 57.0 , Loss:  2.5e-05\n",
            "Epoch №: 1 , percent of batch: 60.0 , Loss:  2.9e-05\n",
            "Epoch №: 1 , percent of batch: 63.0 , Loss:  4.1e-05\n",
            "Epoch №: 1 , percent of batch: 67.0 , Loss:  7.9e-05\n",
            "Epoch №: 1 , percent of batch: 70.0 , Loss:  4.3e-05\n",
            "Epoch №: 1 , percent of batch: 73.0 , Loss:  3.3e-05\n",
            "Epoch №: 1 , percent of batch: 77.0 , Loss:  9.2e-05\n",
            "Epoch №: 1 , percent of batch: 80.0 , Loss:  2.5e-05\n",
            "Epoch №: 1 , percent of batch: 83.0 , Loss:  4e-05\n",
            "Epoch №: 1 , percent of batch: 87.0 , Loss:  4.2e-05\n",
            "Epoch №: 1 , percent of batch: 90.0 , Loss:  5.2e-05\n",
            "Epoch №: 1 , percent of batch: 93.0 , Loss:  6.7e-05\n",
            "Epoch №: 1 , percent of batch: 97.0 , Loss:  6.7e-05\n",
            "Epoch №: 2 , percent of batch: 0.0 , Loss:  3e-05\n",
            "Epoch №: 2 , percent of batch: 3.0 , Loss:  4e-05\n",
            "Epoch №: 2 , percent of batch: 7.0 , Loss:  5.4e-05\n",
            "Epoch №: 2 , percent of batch: 10.0 , Loss:  1.9e-05\n",
            "Epoch №: 2 , percent of batch: 13.0 , Loss:  2.8e-05\n",
            "Epoch №: 2 , percent of batch: 17.0 , Loss:  8.7e-05\n",
            "Epoch №: 2 , percent of batch: 20.0 , Loss:  5.3e-05\n",
            "Epoch №: 2 , percent of batch: 23.0 , Loss:  5.4e-05\n",
            "Epoch №: 2 , percent of batch: 27.0 , Loss:  0.000118\n",
            "Epoch №: 2 , percent of batch: 30.0 , Loss:  0.0001\n",
            "Epoch №: 2 , percent of batch: 33.0 , Loss:  2e-05\n",
            "Epoch №: 2 , percent of batch: 37.0 , Loss:  9.3e-05\n",
            "Epoch №: 2 , percent of batch: 40.0 , Loss:  5.2e-05\n",
            "Epoch №: 2 , percent of batch: 43.0 , Loss:  3.3e-05\n",
            "Epoch №: 2 , percent of batch: 47.0 , Loss:  4.9e-05\n",
            "Epoch №: 2 , percent of batch: 50.0 , Loss:  7.8e-05\n",
            "Epoch №: 2 , percent of batch: 53.0 , Loss:  3.3e-05\n",
            "Epoch №: 2 , percent of batch: 57.0 , Loss:  6e-05\n",
            "Epoch №: 2 , percent of batch: 60.0 , Loss:  5e-05\n",
            "Epoch №: 2 , percent of batch: 63.0 , Loss:  6.5e-05\n",
            "Epoch №: 2 , percent of batch: 67.0 , Loss:  5.9e-05\n",
            "Epoch №: 2 , percent of batch: 70.0 , Loss:  6.5e-05\n",
            "Epoch №: 2 , percent of batch: 73.0 , Loss:  7.8e-05\n",
            "Epoch №: 2 , percent of batch: 77.0 , Loss:  6.5e-05\n",
            "Epoch №: 2 , percent of batch: 80.0 , Loss:  9.6e-05\n",
            "Epoch №: 2 , percent of batch: 83.0 , Loss:  4e-05\n",
            "Epoch №: 2 , percent of batch: 87.0 , Loss:  4.9e-05\n",
            "Epoch №: 2 , percent of batch: 90.0 , Loss:  4.4e-05\n",
            "Epoch №: 2 , percent of batch: 93.0 , Loss:  7e-05\n",
            "Epoch №: 2 , percent of batch: 97.0 , Loss:  4.6e-05\n",
            "Epoch №: 3 , percent of batch: 0.0 , Loss:  4.9e-05\n",
            "Epoch №: 3 , percent of batch: 3.0 , Loss:  3.3e-05\n",
            "Epoch №: 3 , percent of batch: 7.0 , Loss:  4.1e-05\n",
            "Epoch №: 3 , percent of batch: 10.0 , Loss:  6.1e-05\n",
            "Epoch №: 3 , percent of batch: 13.0 , Loss:  8.6e-05\n",
            "Epoch №: 3 , percent of batch: 17.0 , Loss:  7.2e-05\n",
            "Epoch №: 3 , percent of batch: 20.0 , Loss:  6.3e-05\n",
            "Epoch №: 3 , percent of batch: 23.0 , Loss:  3.7e-05\n",
            "Epoch №: 3 , percent of batch: 27.0 , Loss:  6.1e-05\n",
            "Epoch №: 3 , percent of batch: 30.0 , Loss:  6.6e-05\n",
            "Epoch №: 3 , percent of batch: 33.0 , Loss:  4e-05\n",
            "Epoch №: 3 , percent of batch: 37.0 , Loss:  3.5e-05\n",
            "Epoch №: 3 , percent of batch: 40.0 , Loss:  6.3e-05\n",
            "Epoch №: 3 , percent of batch: 43.0 , Loss:  3e-05\n",
            "Epoch №: 3 , percent of batch: 47.0 , Loss:  3.6e-05\n",
            "Epoch №: 3 , percent of batch: 50.0 , Loss:  4.4e-05\n",
            "Epoch №: 3 , percent of batch: 53.0 , Loss:  2.8e-05\n",
            "Epoch №: 3 , percent of batch: 57.0 , Loss:  4.5e-05\n",
            "Epoch №: 3 , percent of batch: 60.0 , Loss:  7.8e-05\n",
            "Epoch №: 3 , percent of batch: 63.0 , Loss:  2.4e-05\n",
            "Epoch №: 3 , percent of batch: 67.0 , Loss:  3.3e-05\n",
            "Epoch №: 3 , percent of batch: 70.0 , Loss:  1.9e-05\n",
            "Epoch №: 3 , percent of batch: 73.0 , Loss:  2.9e-05\n",
            "Epoch №: 3 , percent of batch: 77.0 , Loss:  1.7e-05\n",
            "Epoch №: 3 , percent of batch: 80.0 , Loss:  5.6e-05\n",
            "Epoch №: 3 , percent of batch: 83.0 , Loss:  5.9e-05\n",
            "Epoch №: 3 , percent of batch: 87.0 , Loss:  6.6e-05\n",
            "Epoch №: 3 , percent of batch: 90.0 , Loss:  4.7e-05\n",
            "Epoch №: 3 , percent of batch: 93.0 , Loss:  4.4e-05\n",
            "Epoch №: 3 , percent of batch: 97.0 , Loss:  5.2e-05\n",
            "Epoch №: 4 , percent of batch: 0.0 , Loss:  6.2e-05\n",
            "Epoch №: 4 , percent of batch: 3.0 , Loss:  2.6e-05\n",
            "Epoch №: 4 , percent of batch: 7.0 , Loss:  3.6e-05\n",
            "Epoch №: 4 , percent of batch: 10.0 , Loss:  3.1e-05\n",
            "Epoch №: 4 , percent of batch: 13.0 , Loss:  9.1e-05\n",
            "Epoch №: 4 , percent of batch: 17.0 , Loss:  5.2e-05\n",
            "Epoch №: 4 , percent of batch: 20.0 , Loss:  4.1e-05\n",
            "Epoch №: 4 , percent of batch: 23.0 , Loss:  5.1e-05\n",
            "Epoch №: 4 , percent of batch: 27.0 , Loss:  4.1e-05\n",
            "Epoch №: 4 , percent of batch: 30.0 , Loss:  5.9e-05\n",
            "Epoch №: 4 , percent of batch: 33.0 , Loss:  3.2e-05\n",
            "Epoch №: 4 , percent of batch: 37.0 , Loss:  7.6e-05\n",
            "Epoch №: 4 , percent of batch: 40.0 , Loss:  4.8e-05\n",
            "Epoch №: 4 , percent of batch: 43.0 , Loss:  4.9e-05\n",
            "Epoch №: 4 , percent of batch: 47.0 , Loss:  5.5e-05\n",
            "Epoch №: 4 , percent of batch: 50.0 , Loss:  4.3e-05\n",
            "Epoch №: 4 , percent of batch: 53.0 , Loss:  4.4e-05\n",
            "Epoch №: 4 , percent of batch: 57.0 , Loss:  5.7e-05\n",
            "Epoch №: 4 , percent of batch: 60.0 , Loss:  2.4e-05\n",
            "Epoch №: 4 , percent of batch: 63.0 , Loss:  2.5e-05\n",
            "Epoch №: 4 , percent of batch: 67.0 , Loss:  6.4e-05\n",
            "Epoch №: 4 , percent of batch: 70.0 , Loss:  4.2e-05\n",
            "Epoch №: 4 , percent of batch: 73.0 , Loss:  4.7e-05\n",
            "Epoch №: 4 , percent of batch: 77.0 , Loss:  4.5e-05\n",
            "Epoch №: 4 , percent of batch: 80.0 , Loss:  5.6e-05\n",
            "Epoch №: 4 , percent of batch: 83.0 , Loss:  3.1e-05\n",
            "Epoch №: 4 , percent of batch: 87.0 , Loss:  9.4e-05\n",
            "Epoch №: 4 , percent of batch: 90.0 , Loss:  2.5e-05\n",
            "Epoch №: 4 , percent of batch: 93.0 , Loss:  3.1e-05\n",
            "Epoch №: 4 , percent of batch: 97.0 , Loss:  6e-05\n",
            "Epoch №: 5 , percent of batch: 0.0 , Loss:  3.7e-05\n",
            "Epoch №: 5 , percent of batch: 3.0 , Loss:  7.2e-05\n",
            "Epoch №: 5 , percent of batch: 7.0 , Loss:  1.1e-05\n",
            "Epoch №: 5 , percent of batch: 10.0 , Loss:  7e-05\n",
            "Epoch №: 5 , percent of batch: 13.0 , Loss:  2.2e-05\n",
            "Epoch №: 5 , percent of batch: 17.0 , Loss:  3.9e-05\n",
            "Epoch №: 5 , percent of batch: 20.0 , Loss:  4.1e-05\n",
            "Epoch №: 5 , percent of batch: 23.0 , Loss:  3.9e-05\n",
            "Epoch №: 5 , percent of batch: 27.0 , Loss:  3.1e-05\n",
            "Epoch №: 5 , percent of batch: 30.0 , Loss:  3e-05\n",
            "Epoch №: 5 , percent of batch: 33.0 , Loss:  2.4e-05\n",
            "Epoch №: 5 , percent of batch: 37.0 , Loss:  9.7e-05\n",
            "Epoch №: 5 , percent of batch: 40.0 , Loss:  6.6e-05\n",
            "Epoch №: 5 , percent of batch: 43.0 , Loss:  6.9e-05\n",
            "Epoch №: 5 , percent of batch: 47.0 , Loss:  8.8e-05\n",
            "Epoch №: 5 , percent of batch: 50.0 , Loss:  3.2e-05\n",
            "Epoch №: 5 , percent of batch: 53.0 , Loss:  1.6e-05\n",
            "Epoch №: 5 , percent of batch: 57.0 , Loss:  3e-05\n",
            "Epoch №: 5 , percent of batch: 60.0 , Loss:  4.1e-05\n",
            "Epoch №: 5 , percent of batch: 63.0 , Loss:  4.4e-05\n",
            "Epoch №: 5 , percent of batch: 67.0 , Loss:  4.7e-05\n",
            "Epoch №: 5 , percent of batch: 70.0 , Loss:  5e-05\n",
            "Epoch №: 5 , percent of batch: 73.0 , Loss:  4.7e-05\n",
            "Epoch №: 5 , percent of batch: 77.0 , Loss:  2.1e-05\n",
            "Epoch №: 5 , percent of batch: 80.0 , Loss:  5.7e-05\n",
            "Epoch №: 5 , percent of batch: 83.0 , Loss:  6.6e-05\n",
            "Epoch №: 5 , percent of batch: 87.0 , Loss:  1.7e-05\n",
            "Epoch №: 5 , percent of batch: 90.0 , Loss:  0.000148\n",
            "Epoch №: 5 , percent of batch: 93.0 , Loss:  8.2e-05\n",
            "Epoch №: 5 , percent of batch: 97.0 , Loss:  5.5e-05\n",
            "Epoch №: 6 , percent of batch: 0.0 , Loss:  3.1e-05\n",
            "Epoch №: 6 , percent of batch: 3.0 , Loss:  3.5e-05\n",
            "Epoch №: 6 , percent of batch: 7.0 , Loss:  6.1e-05\n",
            "Epoch №: 6 , percent of batch: 10.0 , Loss:  4.5e-05\n",
            "Epoch №: 6 , percent of batch: 13.0 , Loss:  3.3e-05\n",
            "Epoch №: 6 , percent of batch: 17.0 , Loss:  3.8e-05\n",
            "Epoch №: 6 , percent of batch: 20.0 , Loss:  6.2e-05\n",
            "Epoch №: 6 , percent of batch: 23.0 , Loss:  6.9e-05\n",
            "Epoch №: 6 , percent of batch: 27.0 , Loss:  4e-05\n",
            "Epoch №: 6 , percent of batch: 30.0 , Loss:  5.9e-05\n",
            "Epoch №: 6 , percent of batch: 33.0 , Loss:  5.6e-05\n",
            "Epoch №: 6 , percent of batch: 37.0 , Loss:  2.9e-05\n",
            "Epoch №: 6 , percent of batch: 40.0 , Loss:  3.8e-05\n",
            "Epoch №: 6 , percent of batch: 43.0 , Loss:  4.3e-05\n",
            "Epoch №: 6 , percent of batch: 47.0 , Loss:  6.7e-05\n",
            "Epoch №: 6 , percent of batch: 50.0 , Loss:  1.4e-05\n",
            "Epoch №: 6 , percent of batch: 53.0 , Loss:  6.1e-05\n",
            "Epoch №: 6 , percent of batch: 57.0 , Loss:  3.1e-05\n",
            "Epoch №: 6 , percent of batch: 60.0 , Loss:  4.2e-05\n",
            "Epoch №: 6 , percent of batch: 63.0 , Loss:  0.000113\n",
            "Epoch №: 6 , percent of batch: 67.0 , Loss:  4.1e-05\n",
            "Epoch №: 6 , percent of batch: 70.0 , Loss:  2.7e-05\n",
            "Epoch №: 6 , percent of batch: 73.0 , Loss:  3.4e-05\n",
            "Epoch №: 6 , percent of batch: 77.0 , Loss:  2.1e-05\n",
            "Epoch №: 6 , percent of batch: 80.0 , Loss:  2.1e-05\n",
            "Epoch №: 6 , percent of batch: 83.0 , Loss:  3.8e-05\n",
            "Epoch №: 6 , percent of batch: 87.0 , Loss:  6.3e-05\n",
            "Epoch №: 6 , percent of batch: 90.0 , Loss:  4.7e-05\n",
            "Epoch №: 6 , percent of batch: 93.0 , Loss:  4.3e-05\n",
            "Epoch №: 6 , percent of batch: 97.0 , Loss:  8.2e-05\n",
            "Epoch №: 7 , percent of batch: 0.0 , Loss:  2e-05\n",
            "Epoch №: 7 , percent of batch: 3.0 , Loss:  4e-05\n",
            "Epoch №: 7 , percent of batch: 7.0 , Loss:  4e-05\n",
            "Epoch №: 7 , percent of batch: 10.0 , Loss:  5.8e-05\n",
            "Epoch №: 7 , percent of batch: 13.0 , Loss:  2e-05\n",
            "Epoch №: 7 , percent of batch: 17.0 , Loss:  2.8e-05\n",
            "Epoch №: 7 , percent of batch: 20.0 , Loss:  4.4e-05\n",
            "Epoch №: 7 , percent of batch: 23.0 , Loss:  5.3e-05\n",
            "Epoch №: 7 , percent of batch: 27.0 , Loss:  4e-05\n",
            "Epoch №: 7 , percent of batch: 30.0 , Loss:  7.4e-05\n",
            "Epoch №: 7 , percent of batch: 33.0 , Loss:  3.6e-05\n",
            "Epoch №: 7 , percent of batch: 37.0 , Loss:  3.7e-05\n",
            "Epoch №: 7 , percent of batch: 40.0 , Loss:  9.7e-05\n",
            "Epoch №: 7 , percent of batch: 43.0 , Loss:  6.5e-05\n",
            "Epoch №: 7 , percent of batch: 47.0 , Loss:  4.3e-05\n",
            "Epoch №: 7 , percent of batch: 50.0 , Loss:  8.7e-05\n",
            "Epoch №: 7 , percent of batch: 53.0 , Loss:  4.1e-05\n",
            "Epoch №: 7 , percent of batch: 57.0 , Loss:  2.9e-05\n",
            "Epoch №: 7 , percent of batch: 60.0 , Loss:  4e-05\n",
            "Epoch №: 7 , percent of batch: 63.0 , Loss:  7e-05\n",
            "Epoch №: 7 , percent of batch: 67.0 , Loss:  3.2e-05\n",
            "Epoch №: 7 , percent of batch: 70.0 , Loss:  5.9e-05\n",
            "Epoch №: 7 , percent of batch: 73.0 , Loss:  5.6e-05\n",
            "Epoch №: 7 , percent of batch: 77.0 , Loss:  1.7e-05\n",
            "Epoch №: 7 , percent of batch: 80.0 , Loss:  3e-05\n",
            "Epoch №: 7 , percent of batch: 83.0 , Loss:  0.000103\n",
            "Epoch №: 7 , percent of batch: 87.0 , Loss:  4.3e-05\n",
            "Epoch №: 7 , percent of batch: 90.0 , Loss:  1.5e-05\n",
            "Epoch №: 7 , percent of batch: 93.0 , Loss:  6.4e-05\n",
            "Epoch №: 7 , percent of batch: 97.0 , Loss:  3e-05\n",
            "Epoch №: 8 , percent of batch: 0.0 , Loss:  4.2e-05\n",
            "Epoch №: 8 , percent of batch: 3.0 , Loss:  9.6e-05\n",
            "Epoch №: 8 , percent of batch: 7.0 , Loss:  1.2e-05\n",
            "Epoch №: 8 , percent of batch: 10.0 , Loss:  3.5e-05\n",
            "Epoch №: 8 , percent of batch: 13.0 , Loss:  3.6e-05\n",
            "Epoch №: 8 , percent of batch: 17.0 , Loss:  6.2e-05\n",
            "Epoch №: 8 , percent of batch: 20.0 , Loss:  5.8e-05\n",
            "Epoch №: 8 , percent of batch: 23.0 , Loss:  2.9e-05\n",
            "Epoch №: 8 , percent of batch: 27.0 , Loss:  1.7e-05\n",
            "Epoch №: 8 , percent of batch: 30.0 , Loss:  4.4e-05\n",
            "Epoch №: 8 , percent of batch: 33.0 , Loss:  7.1e-05\n",
            "Epoch №: 8 , percent of batch: 37.0 , Loss:  4e-05\n",
            "Epoch №: 8 , percent of batch: 40.0 , Loss:  3.1e-05\n",
            "Epoch №: 8 , percent of batch: 43.0 , Loss:  3.3e-05\n",
            "Epoch №: 8 , percent of batch: 47.0 , Loss:  1.4e-05\n",
            "Epoch №: 8 , percent of batch: 50.0 , Loss:  7.9e-05\n",
            "Epoch №: 8 , percent of batch: 53.0 , Loss:  7.8e-05\n",
            "Epoch №: 8 , percent of batch: 57.0 , Loss:  4.6e-05\n",
            "Epoch №: 8 , percent of batch: 60.0 , Loss:  5.3e-05\n",
            "Epoch №: 8 , percent of batch: 63.0 , Loss:  3.8e-05\n",
            "Epoch №: 8 , percent of batch: 67.0 , Loss:  2.8e-05\n",
            "Epoch №: 8 , percent of batch: 70.0 , Loss:  5.2e-05\n",
            "Epoch №: 8 , percent of batch: 73.0 , Loss:  5.1e-05\n",
            "Epoch №: 8 , percent of batch: 77.0 , Loss:  2.7e-05\n",
            "Epoch №: 8 , percent of batch: 80.0 , Loss:  4.3e-05\n",
            "Epoch №: 8 , percent of batch: 83.0 , Loss:  2.5e-05\n",
            "Epoch №: 8 , percent of batch: 87.0 , Loss:  3.8e-05\n",
            "Epoch №: 8 , percent of batch: 90.0 , Loss:  4.7e-05\n",
            "Epoch №: 8 , percent of batch: 93.0 , Loss:  6.4e-05\n",
            "Epoch №: 8 , percent of batch: 97.0 , Loss:  2.5e-05\n",
            "Epoch №: 9 , percent of batch: 0.0 , Loss:  4.5e-05\n",
            "Epoch №: 9 , percent of batch: 3.0 , Loss:  5.2e-05\n",
            "Epoch №: 9 , percent of batch: 7.0 , Loss:  5.5e-05\n",
            "Epoch №: 9 , percent of batch: 10.0 , Loss:  7.1e-05\n",
            "Epoch №: 9 , percent of batch: 13.0 , Loss:  5.1e-05\n",
            "Epoch №: 9 , percent of batch: 17.0 , Loss:  5.7e-05\n",
            "Epoch №: 9 , percent of batch: 20.0 , Loss:  3.4e-05\n",
            "Epoch №: 9 , percent of batch: 23.0 , Loss:  4.5e-05\n",
            "Epoch №: 9 , percent of batch: 27.0 , Loss:  6.4e-05\n",
            "Epoch №: 9 , percent of batch: 30.0 , Loss:  4.6e-05\n",
            "Epoch №: 9 , percent of batch: 33.0 , Loss:  4.9e-05\n",
            "Epoch №: 9 , percent of batch: 37.0 , Loss:  9.1e-05\n",
            "Epoch №: 9 , percent of batch: 40.0 , Loss:  4.3e-05\n",
            "Epoch №: 9 , percent of batch: 43.0 , Loss:  3.3e-05\n",
            "Epoch №: 9 , percent of batch: 47.0 , Loss:  3.7e-05\n",
            "Epoch №: 9 , percent of batch: 50.0 , Loss:  2.8e-05\n",
            "Epoch №: 9 , percent of batch: 53.0 , Loss:  2.4e-05\n",
            "Epoch №: 9 , percent of batch: 57.0 , Loss:  5.2e-05\n",
            "Epoch №: 9 , percent of batch: 60.0 , Loss:  3.7e-05\n",
            "Epoch №: 9 , percent of batch: 63.0 , Loss:  3.8e-05\n",
            "Epoch №: 9 , percent of batch: 67.0 , Loss:  9e-05\n",
            "Epoch №: 9 , percent of batch: 70.0 , Loss:  8.4e-05\n",
            "Epoch №: 9 , percent of batch: 73.0 , Loss:  1.3e-05\n",
            "Epoch №: 9 , percent of batch: 77.0 , Loss:  4.5e-05\n",
            "Epoch №: 9 , percent of batch: 80.0 , Loss:  5.4e-05\n",
            "Epoch №: 9 , percent of batch: 83.0 , Loss:  5.8e-05\n",
            "Epoch №: 9 , percent of batch: 87.0 , Loss:  4.2e-05\n",
            "Epoch №: 9 , percent of batch: 90.0 , Loss:  2.5e-05\n",
            "Epoch №: 9 , percent of batch: 93.0 , Loss:  2.4e-05\n",
            "Epoch №: 9 , percent of batch: 97.0 , Loss:  5.7e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsMF_uf8kVq2",
        "outputId": "0e213a8a-1db0-44dd-c9b3-dca8a1e017ff"
      },
      "source": [
        "# now we will test our neural network on train dataset\n",
        "\n",
        "final_loss = 0\n",
        "answ = 0\n",
        "for data, target in test_loader:\n",
        "   data, target = Variable(data, volatile=True), Variable(target)\n",
        "   data = data.view(-1, 28 * 28)\n",
        "   no = linModel(data)\n",
        "   final_loss += criterion(no, target).item()\n",
        "   last = no.data.max(1)[1]\n",
        "   answ += last.eq(target.data).sum()\n",
        "\n",
        "final_loss /= len(test_loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Average loss:\", float('{:.6f}'.format(final_loss)), \", accuracy:\", float('{:.3f}'.format((100. * correct / len(test_loader.dataset)))))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss: 0.000447 , accuracy: 98.36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-vM6V_1mlIj"
      },
      "source": [
        "# and now we will create convolutional neural network for our MNIST dataset\n",
        "\n",
        "class ConvNet(nn.Module): \n",
        "    def __init__(self): \n",
        "        # initilization\n",
        "        super().__init__()\n",
        "        # first layer\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2), \n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) \n",
        "        # second layer\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2), \n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) \n",
        "        # drop out\n",
        "        self.drop_out = nn.Dropout() \n",
        "        # two fully connected layers\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000) \n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward function for our layers \n",
        "        out = self.layer1(x) \n",
        "        out = self.layer2(out) \n",
        "        out = out.reshape(out.size(0), -1) \n",
        "        out = self.drop_out(out) \n",
        "        out = self.fc1(out) \n",
        "        out = self.fc2(out) \n",
        "        return out"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brZYe03To-yd"
      },
      "source": [
        "ConvModel = ConvNet()\n",
        "\n",
        "# create Adam optimization\n",
        "optimizer = torch.optim.Adam(ConvModel.parameters(), lr=0.001)\n",
        "\n",
        "# create loss func\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hRjUiTGpAYm",
        "outputId": "460c1c2c-3398-410a-fa89-4972e47be39d"
      },
      "source": [
        "total_step = len(train_loader)\n",
        "all_losses = []\n",
        "all_acc = []\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # forward step\n",
        "        no = ConvModel(images)\n",
        "        loss = criterion(no, labels)\n",
        "        all_losses.append(loss.item())\n",
        "\n",
        "        # back step and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # accuracy\n",
        "        sum = labels.size(0)\n",
        "        local, expected = torch.max(no.data, 1)\n",
        "        correct = (expected == labels).sum().item()\n",
        "        all_acc.append(correct / sum)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(\"Epoch №:\", epoch + 1, \", percent of batch\", float('{:.2f}'.format((i + 1) / 300)) * 100,\n",
        "                  \", loss:\", loss.item(), \"accuracy:\", float('{:.2f}'.format((correct / sum) * 100)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch №: 1 , percent of batch 33.0 , loss: 0.03571074828505516 accuracy: 98.5\n",
            "Epoch №: 1 , percent of batch 67.0 , loss: 0.015923863276839256 accuracy: 99.5\n",
            "Epoch №: 1 , percent of batch 100.0 , loss: 0.06652771681547165 accuracy: 97.5\n",
            "Epoch №: 2 , percent of batch 33.0 , loss: 0.05505935847759247 accuracy: 98.5\n",
            "Epoch №: 2 , percent of batch 67.0 , loss: 0.04607236385345459 accuracy: 99.5\n",
            "Epoch №: 2 , percent of batch 100.0 , loss: 0.043576039373874664 accuracy: 98.0\n",
            "Epoch №: 3 , percent of batch 33.0 , loss: 0.04587356001138687 accuracy: 98.0\n",
            "Epoch №: 3 , percent of batch 67.0 , loss: 0.02097713015973568 accuracy: 99.0\n",
            "Epoch №: 3 , percent of batch 100.0 , loss: 0.0203156229108572 accuracy: 99.5\n",
            "Epoch №: 4 , percent of batch 33.0 , loss: 0.024467742070555687 accuracy: 99.5\n",
            "Epoch №: 4 , percent of batch 67.0 , loss: 0.05455690994858742 accuracy: 98.5\n",
            "Epoch №: 4 , percent of batch 100.0 , loss: 0.006874406710267067 accuracy: 100.0\n",
            "Epoch №: 5 , percent of batch 33.0 , loss: 0.07815394550561905 accuracy: 98.5\n",
            "Epoch №: 5 , percent of batch 67.0 , loss: 0.030030131340026855 accuracy: 99.5\n",
            "Epoch №: 5 , percent of batch 100.0 , loss: 0.01401408389210701 accuracy: 99.0\n",
            "Epoch №: 6 , percent of batch 33.0 , loss: 0.006185715086758137 accuracy: 100.0\n",
            "Epoch №: 6 , percent of batch 67.0 , loss: 0.0549774132668972 accuracy: 98.5\n",
            "Epoch №: 6 , percent of batch 100.0 , loss: 0.06417495757341385 accuracy: 99.0\n",
            "Epoch №: 7 , percent of batch 33.0 , loss: 0.03044544719159603 accuracy: 99.5\n",
            "Epoch №: 7 , percent of batch 67.0 , loss: 0.045348502695560455 accuracy: 98.0\n",
            "Epoch №: 7 , percent of batch 100.0 , loss: 0.047539159655570984 accuracy: 98.0\n",
            "Epoch №: 8 , percent of batch 33.0 , loss: 0.011894810013473034 accuracy: 99.5\n",
            "Epoch №: 8 , percent of batch 67.0 , loss: 0.014798655174672604 accuracy: 99.5\n",
            "Epoch №: 8 , percent of batch 100.0 , loss: 0.005374772008508444 accuracy: 100.0\n",
            "Epoch №: 9 , percent of batch 33.0 , loss: 0.0066098906099796295 accuracy: 100.0\n",
            "Epoch №: 9 , percent of batch 67.0 , loss: 0.028137153014540672 accuracy: 99.0\n",
            "Epoch №: 9 , percent of batch 100.0 , loss: 0.03517657890915871 accuracy: 99.0\n",
            "Epoch №: 10 , percent of batch 33.0 , loss: 0.020859941840171814 accuracy: 99.5\n",
            "Epoch №: 10 , percent of batch 67.0 , loss: 0.00497083505615592 accuracy: 100.0\n",
            "Epoch №: 10 , percent of batch 100.0 , loss: 0.01640646532177925 accuracy: 99.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYF9vaiIpBdP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}